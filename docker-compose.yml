# Kōan Docker Compose
#
# Claude CLI is installed in the image. Auth options:
#   1. ANTHROPIC_API_KEY in .env (API billing accounts)
#   2. make docker-auth (subscription — generates OAuth token from host CLI)
#
# Run ./setup-docker.sh first — it generates docker-compose.override.yml
# with workspace mounts and GitHub CLI auth for your system.
#
# Usage:
#   ./setup-docker.sh                        # Auto-detect host setup
#   make docker-auth                         # Generate OAuth token from host CLI
#   docker compose up --build                # Full stack (agent + bridge)
#   docker compose run --rm koan test        # Run test suite
#   docker compose run --rm koan shell       # Interactive shell

services:
  koan:
    build:
      context: .
      args:
        HOST_UID: ${HOST_UID:-1000}
        HOST_GID: ${HOST_GID:-1000}
    container_name: koan
    env_file:
      - .env
      - path: .env.docker
        required: false
    environment:
      - KOAN_ROOT=/app
      - PYTHONPATH=/app/koan
    volumes:
      # Instance state — persisted across restarts
      - ./instance:/app/instance
      # Isolated mission queue for Docker (overlays instance/ mount)
      - ./instance/missions.docker.md:/app/instance/missions.md
      # Logs
      - ./logs:/app/logs
    restart: unless-stopped
    # No ports exposed by default — Telegram/Slack only
    # Uncomment to expose dashboard:
    # ports:
    #   - "5001:5001"

  # --------------------------------------------------------------------------
  # Optional: Ollama service for local/ollama/ollama-claude providers.
  # Uncomment the section below to run Ollama inside Docker Compose.
  # For GPU passthrough, add a deploy.resources section (see Ollama docs).
  #
  # Usage:
  #   docker compose --profile ollama up --build
  #
  # Set in .env:
  #   KOAN_CLI_PROVIDER=ollama
  #   KOAN_LOCAL_LLM_BASE_URL=http://ollama:11434/v1
  # --------------------------------------------------------------------------
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: koan-ollama
  #   profiles: ["ollama"]
  #   volumes:
  #     - ollama-models:/root/.ollama
  #   ports:
  #     - "11434:11434"
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:11434/"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5

# volumes:
#   ollama-models:
